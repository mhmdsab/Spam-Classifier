{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM CLASSIFICATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to follow a step by step procedure to create a spam classifier.\n",
    "This spam classifier uses embeddings trained on the spam classification data set using Fasttext library.\n",
    "These embeddings are then fed to bidirectional LSTM layer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate embedding size and maximum words in words sequence (sentence)\n",
    "embedding_size = 50\n",
    "max_words_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe consists of two columns the v1 and v2 and three other empty columns. v2 column contains the mail body while v1 column contain mail class (ham or spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data\n",
    "df = pd.read_csv('C:/Users/User/Downloads/spam.csv', encoding='latin-1').loc[:,['v1','v2']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we clean the data to be ready for training embeddings\n",
    "def preprocess(df_):\n",
    "    df_cleaned = df_.copy()\n",
    "    #remove nan values\n",
    "    df_cleaned.dropna(inplace = True)\n",
    "    #replace any number by the word number\n",
    "    df_cleaned['v2'] = df_cleaned['v2'].str.replace(r'\\d',' number ')\n",
    "    #remove any punctuations\n",
    "    df_cleaned['v2'] = df_cleaned['v2'].str.replace(r'[^a-zA-Z]', ' ', regex = True)\n",
    "    #remove single characters\n",
    "    df_cleaned['v2'] = df_cleaned['v2'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ', regex = True)\n",
    "    #remove extra spaces\n",
    "    df_cleaned['v2'] = df_cleaned['v2'].str.replace(r'\\s+', ' ', regex = True).map(lambda x:x.lower())\n",
    "    return df_cleaned\n",
    "\n",
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the text cleaned. we create a corpus of the entire dataset in a txt file. This corpus will be fed to a skipgram\n",
    "model to train embeddings. The main advantage of training embeddings using fasttext is that it trains on the entire word \n",
    "and its subwords as well. This minimizes the probability of having out of vocabulary words as in such case, this word will \n",
    "be divided into subwords hopefully present in fasttext model subwords. then the word embedding will be the average of its \n",
    "subwords embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus for training embeddings\n",
    "with open(r'C:\\Users\\User\\Downloads\\ftw\\spamcorpus.txt', 'w', encoding=\"latin-1\") as txtfile:\n",
    "    for i in range(len(df)):\n",
    "        line = df.loc[i,'v2']\n",
    "        txtfile.write(line)\n",
    "        txtfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and train skipgram model using your own custom configurtions\n",
    "model = fasttext.train_unsupervised('C:/Users/User/Downloads/ftw/spamcorpus.txt',\n",
    "                                    minCount = 5, \n",
    "                                    model='skipgram',\n",
    "                                    minn = 2,\n",
    "                                    maxn = 5,\n",
    "                                    dim = embedding_size,\n",
    "                                    lr = 0.1,\n",
    "                                    epoch = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a txt file that contains every unique word in the dataset and its embeddings. This can be done by selecting all unique word in the dataset. Then using the trained skigram model, we can have the embeddings for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all unique words in the dataset\n",
    "with open(r'C:\\Users\\User\\Downloads\\ftw\\spamcorpus.txt', 'r', encoding=\"utf-8\") as txtfile:\n",
    "    corpus_sentences = txtfile.readlines()\n",
    "    corpus_words = []\n",
    "    for sent in corpus_sentences:\n",
    "        tokenized_sent = sent.split()\n",
    "        for word_ in tokenized_sent:\n",
    "            corpus_words.append(word_)\n",
    "            \n",
    "    corpus_unique_words = list(set(corpus_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embedding txt file\n",
    "with open(r'C:\\Users\\User\\Downloads\\ftw\\fasttext_embeddings.txt', 'w', encoding=\"utf-8\") as txtfile:\n",
    "    txtfile.write(str(len(corpus_unique_words)) + \" \" + str(model.get_dimension()))\n",
    "    txtfile.write('\\n')\n",
    "    for word in corpus_unique_words:\n",
    "        embedding = model.get_word_vector(word)\n",
    "        vstr = \"\"\n",
    "        for vi in embedding:\n",
    "            vstr += \" \" + str(vi)\n",
    "        txtfile.write(word + vstr)\n",
    "        txtfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the previous steps were done to create the words embeddings txt file.\n",
    "As this file is ready now, the data preparation steps will be as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Create an embedding dictionary (keys are unique words, values are embedding arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create a keras tokenizer and fit it on the cleaned text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted tokenizer now has a dictionary of every unique word and its index in a randomly initialized embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: We will create embedding matrix from the created embedding dictionary and will use it instead of the randomly initialized embedding matrix. we will assign every word embedding with its index in the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding matrix is now ready and will be fed directly to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: The tokenizer converts each sequence of words to a sequence of their indices in both tokenizer and embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: sequences with more words than maximum words length (50 in this notebook) are truncated, whereas sequences with less words are padded to maximum words length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: Create an embedding dictionary \n",
    "embedding_dictionary = dict()\n",
    "\n",
    "with open(r'C:\\Users\\User\\Downloads\\ftw\\fasttext_embeddings.txt', 'r', encoding=\"utf-8\") as txtfile:\n",
    "    embeddings = txtfile.readlines()[1:]\n",
    "    for line in embeddings:\n",
    "        x = line.split()\n",
    "        word = x[0]\n",
    "        embeds = np.asarray(x[1:]).astype(np.float32)\n",
    "        embedding_dictionary[word] = embeds\n",
    "    embedding_dictionary['UNK'] = np.mean(list(embedding_dictionary.values()), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Create a keras tokenizer and fit it on the cleaned text.\n",
    "num_words = len(corpus_unique_words)\n",
    "tokenizer = Tokenizer(num_words+1, oov_token = 'UNK')\n",
    "tokenizer.fit_on_texts(df['v2'])\n",
    "\n",
    "#Note1: the number of words in the tokenizer is 1 indexed (index starts from 1)\n",
    "#Note2: we add 1 to the number of words in the tokenizer as it includes the unknown token \n",
    "#Note3: we don't have to add the total number of unique words in the tokenizer, \n",
    "#if we use less number, the tokenizer will account for only the top frequent n words we enter\n",
    "#but I added the total number of words as every single word now hopefully has a meaningful embedding thanks to fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 7704/7704 [00:00<00:00, 405167.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#Step 3: We will create embedding matrix from the created embedding dictionary\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "embeddings_matrix = np.zeros(shape = (vocab_size , embedding_size))\n",
    "\n",
    "for word, index in tqdm(tokenizer.word_index.items()):\n",
    "    embeddings_matrix[index] = embedding_dictionary.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: The tokenizer converts each sequence of words to a sequence of their indices in both tokenizer and embeddings\n",
    "X = tokenizer.texts_to_sequences(df['v2'])\n",
    "#step 5: padding short sequences and truncating long sequences\n",
    "X = pad_sequences(X, padding = 'post', maxlen = max_words_len, truncating='post')\n",
    "#encoding labels\n",
    "Y = pd.get_dummies(df['v1'])['spam'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#create a bidirectional LSTM model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_size, embedding_size, \n",
    "                                weights=[embeddings_matrix], \n",
    "                                input_length=max_words_len , \n",
    "                                trainable=True)\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor= 'val_acc', \n",
    "                               mode = 'max',\n",
    "                               patience=30, \n",
    "                               verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('SPAM_CLASSIFIER',\n",
    "                                   monitor = 'val_acc', \n",
    "                                   mode = 'max', \n",
    "                                   save_best_only=True, \n",
    "                                   verbose=1)\n",
    "\n",
    "\n",
    "opt = Adam(lr = 0.01)\n",
    "\n",
    "model.compile(opt, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5014 samples, validate on 558 samples\n",
      "Epoch 1/200\n",
      "4992/5014 [============================>.] - ETA: 2:21 - loss: 0.7011 - acc: 0.406 - ETA: 1:15 - loss: 0.5801 - acc: 0.656 - ETA: 53s - loss: 0.4445 - acc: 0.770 - ETA: 43s - loss: 0.3955 - acc: 0.82 - ETA: 36s - loss: 0.3712 - acc: 0.83 - ETA: 31s - loss: 0.3237 - acc: 0.85 - ETA: 28s - loss: 0.2948 - acc: 0.87 - ETA: 26s - loss: 0.2801 - acc: 0.89 - ETA: 24s - loss: 0.2568 - acc: 0.90 - ETA: 22s - loss: 0.2503 - acc: 0.90 - ETA: 21s - loss: 0.2383 - acc: 0.90 - ETA: 20s - loss: 0.2245 - acc: 0.91 - ETA: 19s - loss: 0.2104 - acc: 0.91 - ETA: 18s - loss: 0.1995 - acc: 0.92 - ETA: 18s - loss: 0.1902 - acc: 0.92 - ETA: 17s - loss: 0.1802 - acc: 0.93 - ETA: 16s - loss: 0.1809 - acc: 0.93 - ETA: 16s - loss: 0.1733 - acc: 0.93 - ETA: 15s - loss: 0.1655 - acc: 0.93 - ETA: 15s - loss: 0.1633 - acc: 0.93 - ETA: 15s - loss: 0.1581 - acc: 0.94 - ETA: 14s - loss: 0.1522 - acc: 0.94 - ETA: 14s - loss: 0.1503 - acc: 0.94 - ETA: 14s - loss: 0.1481 - acc: 0.94 - ETA: 13s - loss: 0.1476 - acc: 0.94 - ETA: 13s - loss: 0.1481 - acc: 0.94 - ETA: 13s - loss: 0.1431 - acc: 0.94 - ETA: 12s - loss: 0.1435 - acc: 0.94 - ETA: 12s - loss: 0.1432 - acc: 0.94 - ETA: 12s - loss: 0.1413 - acc: 0.94 - ETA: 12s - loss: 0.1386 - acc: 0.94 - ETA: 12s - loss: 0.1384 - acc: 0.94 - ETA: 11s - loss: 0.1374 - acc: 0.94 - ETA: 11s - loss: 0.1336 - acc: 0.94 - ETA: 11s - loss: 0.1301 - acc: 0.95 - ETA: 11s - loss: 0.1311 - acc: 0.95 - ETA: 11s - loss: 0.1277 - acc: 0.95 - ETA: 10s - loss: 0.1245 - acc: 0.95 - ETA: 10s - loss: 0.1259 - acc: 0.95 - ETA: 10s - loss: 0.1235 - acc: 0.95 - ETA: 10s - loss: 0.1225 - acc: 0.95 - ETA: 10s - loss: 0.1244 - acc: 0.95 - ETA: 10s - loss: 0.1265 - acc: 0.95 - ETA: 9s - loss: 0.1245 - acc: 0.9545 - ETA: 9s - loss: 0.1225 - acc: 0.955 - ETA: 9s - loss: 0.1204 - acc: 0.956 - ETA: 9s - loss: 0.1185 - acc: 0.957 - ETA: 9s - loss: 0.1187 - acc: 0.956 - ETA: 9s - loss: 0.1174 - acc: 0.956 - ETA: 9s - loss: 0.1181 - acc: 0.955 - ETA: 9s - loss: 0.1165 - acc: 0.955 - ETA: 9s - loss: 0.1150 - acc: 0.956 - ETA: 9s - loss: 0.1131 - acc: 0.957 - ETA: 8s - loss: 0.1112 - acc: 0.958 - ETA: 8s - loss: 0.1093 - acc: 0.959 - ETA: 8s - loss: 0.1082 - acc: 0.959 - ETA: 8s - loss: 0.1111 - acc: 0.958 - ETA: 8s - loss: 0.1094 - acc: 0.959 - ETA: 8s - loss: 0.1079 - acc: 0.960 - ETA: 8s - loss: 0.1079 - acc: 0.960 - ETA: 8s - loss: 0.1065 - acc: 0.961 - ETA: 7s - loss: 0.1052 - acc: 0.961 - ETA: 7s - loss: 0.1040 - acc: 0.961 - ETA: 7s - loss: 0.1032 - acc: 0.961 - ETA: 7s - loss: 0.1017 - acc: 0.962 - ETA: 7s - loss: 0.1002 - acc: 0.963 - ETA: 7s - loss: 0.0988 - acc: 0.963 - ETA: 7s - loss: 0.1001 - acc: 0.963 - ETA: 7s - loss: 0.0987 - acc: 0.963 - ETA: 7s - loss: 0.0977 - acc: 0.963 - ETA: 6s - loss: 0.0964 - acc: 0.964 - ETA: 6s - loss: 0.0952 - acc: 0.964 - ETA: 6s - loss: 0.0940 - acc: 0.965 - ETA: 6s - loss: 0.0928 - acc: 0.965 - ETA: 6s - loss: 0.0916 - acc: 0.966 - ETA: 6s - loss: 0.0905 - acc: 0.966 - ETA: 6s - loss: 0.0893 - acc: 0.967 - ETA: 6s - loss: 0.0889 - acc: 0.967 - ETA: 6s - loss: 0.0878 - acc: 0.967 - ETA: 6s - loss: 0.0870 - acc: 0.968 - ETA: 5s - loss: 0.0860 - acc: 0.968 - ETA: 5s - loss: 0.0867 - acc: 0.968 - ETA: 5s - loss: 0.0857 - acc: 0.968 - ETA: 5s - loss: 0.0847 - acc: 0.969 - ETA: 5s - loss: 0.0837 - acc: 0.969 - ETA: 5s - loss: 0.0828 - acc: 0.969 - ETA: 5s - loss: 0.0819 - acc: 0.970 - ETA: 5s - loss: 0.0813 - acc: 0.970 - ETA: 5s - loss: 0.0804 - acc: 0.970 - ETA: 5s - loss: 0.0795 - acc: 0.970 - ETA: 5s - loss: 0.0787 - acc: 0.971 - ETA: 4s - loss: 0.0779 - acc: 0.971 - ETA: 4s - loss: 0.0772 - acc: 0.971 - ETA: 4s - loss: 0.0764 - acc: 0.972 - ETA: 4s - loss: 0.0756 - acc: 0.972 - ETA: 4s - loss: 0.0771 - acc: 0.972 - ETA: 4s - loss: 0.0763 - acc: 0.972 - ETA: 4s - loss: 0.0758 - acc: 0.972 - ETA: 4s - loss: 0.0751 - acc: 0.972 - ETA: 4s - loss: 0.0744 - acc: 0.973 - ETA: 4s - loss: 0.0737 - acc: 0.973 - ETA: 4s - loss: 0.0730 - acc: 0.973 - ETA: 4s - loss: 0.0723 - acc: 0.973 - ETA: 3s - loss: 0.0749 - acc: 0.973 - ETA: 3s - loss: 0.0743 - acc: 0.973 - ETA: 3s - loss: 0.0740 - acc: 0.973 - ETA: 3s - loss: 0.0736 - acc: 0.973 - ETA: 3s - loss: 0.0732 - acc: 0.973 - ETA: 3s - loss: 0.0727 - acc: 0.973 - ETA: 3s - loss: 0.0729 - acc: 0.973 - ETA: 3s - loss: 0.0724 - acc: 0.973 - ETA: 3s - loss: 0.0723 - acc: 0.973 - ETA: 3s - loss: 0.0718 - acc: 0.974 - ETA: 3s - loss: 0.0714 - acc: 0.974 - ETA: 3s - loss: 0.0709 - acc: 0.974 - ETA: 2s - loss: 0.0705 - acc: 0.974 - ETA: 2s - loss: 0.0700 - acc: 0.974 - ETA: 2s - loss: 0.0695 - acc: 0.975 - ETA: 2s - loss: 0.0690 - acc: 0.975 - ETA: 2s - loss: 0.0684 - acc: 0.975 - ETA: 2s - loss: 0.0691 - acc: 0.975 - ETA: 2s - loss: 0.0687 - acc: 0.975 - ETA: 2s - loss: 0.0681 - acc: 0.975 - ETA: 2s - loss: 0.0676 - acc: 0.976 - ETA: 2s - loss: 0.0672 - acc: 0.976 - ETA: 2s - loss: 0.0667 - acc: 0.976 - ETA: 2s - loss: 0.0662 - acc: 0.976 - ETA: 2s - loss: 0.0657 - acc: 0.976 - ETA: 2s - loss: 0.0652 - acc: 0.977 - ETA: 1s - loss: 0.0655 - acc: 0.976 - ETA: 1s - loss: 0.0657 - acc: 0.976 - ETA: 1s - loss: 0.0653 - acc: 0.977 - ETA: 1s - loss: 0.0650 - acc: 0.977 - ETA: 1s - loss: 0.0647 - acc: 0.977 - ETA: 1s - loss: 0.0645 - acc: 0.977 - ETA: 1s - loss: 0.0654 - acc: 0.977 - ETA: 1s - loss: 0.0649 - acc: 0.977 - ETA: 1s - loss: 0.0645 - acc: 0.977 - ETA: 1s - loss: 0.0643 - acc: 0.977 - ETA: 1s - loss: 0.0640 - acc: 0.977 - ETA: 1s - loss: 0.0636 - acc: 0.977 - ETA: 1s - loss: 0.0633 - acc: 0.978 - ETA: 0s - loss: 0.0629 - acc: 0.978 - ETA: 0s - loss: 0.0640 - acc: 0.978 - ETA: 0s - loss: 0.0637 - acc: 0.978 - ETA: 0s - loss: 0.0633 - acc: 0.978 - ETA: 0s - loss: 0.0629 - acc: 0.978 - ETA: 0s - loss: 0.0625 - acc: 0.978 - ETA: 0s - loss: 0.0621 - acc: 0.978 - ETA: 0s - loss: 0.0618 - acc: 0.979 - ETA: 0s - loss: 0.0614 - acc: 0.979 - ETA: 0s - loss: 0.0610 - acc: 0.979 - ETA: 0s - loss: 0.0618 - acc: 0.979 - ETA: 0s - loss: 0.0614 - acc: 0.979 - ETA: 0s - loss: 0.0610 - acc: 0.979 - ETA: 0s - loss: 0.0609 - acc: 0.9794\n",
      "Epoch 00001: val_acc improved from -inf to 0.98387, saving model to SPAM_CLASSIFIER\n",
      "5014/5014 [==============================] - 12s 2ms/sample - loss: 0.0606 - acc: 0.9795 - val_loss: 0.0650 - val_acc: 0.9839\n",
      "Epoch 2/200\n",
      "4992/5014 [============================>.] - ETA: 8s - loss: 0.0028 - acc: 1.000 - ETA: 8s - loss: 0.0729 - acc: 0.984 - ETA: 8s - loss: 0.0497 - acc: 0.989 - ETA: 8s - loss: 0.0381 - acc: 0.992 - ETA: 8s - loss: 0.0313 - acc: 0.993 - ETA: 8s - loss: 0.0267 - acc: 0.994 - ETA: 8s - loss: 0.0324 - acc: 0.991 - ETA: 8s - loss: 0.0290 - acc: 0.992 - ETA: 8s - loss: 0.0286 - acc: 0.993 - ETA: 8s - loss: 0.0298 - acc: 0.990 - ETA: 8s - loss: 0.0275 - acc: 0.991 - ETA: 8s - loss: 0.0270 - acc: 0.992 - ETA: 8s - loss: 0.0272 - acc: 0.992 - ETA: 8s - loss: 0.0256 - acc: 0.993 - ETA: 8s - loss: 0.0244 - acc: 0.993 - ETA: 8s - loss: 0.0230 - acc: 0.994 - ETA: 8s - loss: 0.0226 - acc: 0.994 - ETA: 8s - loss: 0.0215 - acc: 0.994 - ETA: 8s - loss: 0.0205 - acc: 0.995 - ETA: 8s - loss: 0.0196 - acc: 0.995 - ETA: 8s - loss: 0.0188 - acc: 0.995 - ETA: 8s - loss: 0.0181 - acc: 0.995 - ETA: 8s - loss: 0.0174 - acc: 0.995 - ETA: 7s - loss: 0.0168 - acc: 0.996 - ETA: 7s - loss: 0.0289 - acc: 0.993 - ETA: 7s - loss: 0.0279 - acc: 0.994 - ETA: 7s - loss: 0.0270 - acc: 0.994 - ETA: 7s - loss: 0.0260 - acc: 0.994 - ETA: 7s - loss: 0.0252 - acc: 0.994 - ETA: 7s - loss: 0.0284 - acc: 0.993 - ETA: 7s - loss: 0.0276 - acc: 0.994 - ETA: 7s - loss: 0.0268 - acc: 0.994 - ETA: 7s - loss: 0.0261 - acc: 0.994 - ETA: 7s - loss: 0.0254 - acc: 0.994 - ETA: 7s - loss: 0.0248 - acc: 0.994 - ETA: 7s - loss: 0.0242 - acc: 0.994 - ETA: 7s - loss: 0.0247 - acc: 0.994 - ETA: 7s - loss: 0.0242 - acc: 0.994 - ETA: 7s - loss: 0.0245 - acc: 0.993 - ETA: 6s - loss: 0.0240 - acc: 0.993 - ETA: 6s - loss: 0.0236 - acc: 0.993 - ETA: 6s - loss: 0.0253 - acc: 0.993 - ETA: 6s - loss: 0.0248 - acc: 0.993 - ETA: 6s - loss: 0.0244 - acc: 0.993 - ETA: 6s - loss: 0.0240 - acc: 0.993 - ETA: 6s - loss: 0.0236 - acc: 0.993 - ETA: 6s - loss: 0.0234 - acc: 0.994 - ETA: 6s - loss: 0.0230 - acc: 0.994 - ETA: 6s - loss: 0.0226 - acc: 0.994 - ETA: 6s - loss: 0.0224 - acc: 0.994 - ETA: 6s - loss: 0.0220 - acc: 0.994 - ETA: 6s - loss: 0.0216 - acc: 0.994 - ETA: 6s - loss: 0.0213 - acc: 0.994 - ETA: 6s - loss: 0.0212 - acc: 0.994 - ETA: 6s - loss: 0.0209 - acc: 0.994 - ETA: 6s - loss: 0.0207 - acc: 0.995 - ETA: 5s - loss: 0.0212 - acc: 0.994 - ETA: 5s - loss: 0.0209 - acc: 0.994 - ETA: 5s - loss: 0.0206 - acc: 0.994 - ETA: 5s - loss: 0.0202 - acc: 0.994 - ETA: 5s - loss: 0.0200 - acc: 0.994 - ETA: 5s - loss: 0.0197 - acc: 0.995 - ETA: 5s - loss: 0.0194 - acc: 0.995 - ETA: 5s - loss: 0.0192 - acc: 0.995 - ETA: 5s - loss: 0.0190 - acc: 0.995 - ETA: 5s - loss: 0.0188 - acc: 0.995 - ETA: 5s - loss: 0.0186 - acc: 0.995 - ETA: 5s - loss: 0.0184 - acc: 0.995 - ETA: 5s - loss: 0.0204 - acc: 0.995 - ETA: 5s - loss: 0.0201 - acc: 0.995 - ETA: 5s - loss: 0.0199 - acc: 0.995 - ETA: 5s - loss: 0.0198 - acc: 0.995 - ETA: 5s - loss: 0.0197 - acc: 0.995 - ETA: 4s - loss: 0.0195 - acc: 0.995 - ETA: 4s - loss: 0.0195 - acc: 0.995 - ETA: 4s - loss: 0.0193 - acc: 0.995 - ETA: 4s - loss: 0.0191 - acc: 0.995 - ETA: 4s - loss: 0.0189 - acc: 0.995 - ETA: 4s - loss: 0.0187 - acc: 0.995 - ETA: 4s - loss: 0.0185 - acc: 0.995 - ETA: 4s - loss: 0.0193 - acc: 0.995 - ETA: 4s - loss: 0.0191 - acc: 0.995 - ETA: 4s - loss: 0.0189 - acc: 0.995 - ETA: 4s - loss: 0.0188 - acc: 0.995 - ETA: 4s - loss: 0.0186 - acc: 0.995 - ETA: 4s - loss: 0.0184 - acc: 0.995 - ETA: 4s - loss: 0.0182 - acc: 0.995 - ETA: 4s - loss: 0.0181 - acc: 0.995 - ETA: 4s - loss: 0.0179 - acc: 0.995 - ETA: 4s - loss: 0.0178 - acc: 0.995 - ETA: 3s - loss: 0.0176 - acc: 0.995 - ETA: 3s - loss: 0.0174 - acc: 0.995 - ETA: 3s - loss: 0.0172 - acc: 0.996 - ETA: 3s - loss: 0.0171 - acc: 0.996 - ETA: 3s - loss: 0.0169 - acc: 0.996 - ETA: 3s - loss: 0.0168 - acc: 0.996 - ETA: 3s - loss: 0.0180 - acc: 0.995 - ETA: 3s - loss: 0.0178 - acc: 0.995 - ETA: 3s - loss: 0.0176 - acc: 0.995 - ETA: 3s - loss: 0.0175 - acc: 0.995 - ETA: 3s - loss: 0.0182 - acc: 0.995 - ETA: 3s - loss: 0.0180 - acc: 0.995 - ETA: 3s - loss: 0.0178 - acc: 0.995 - ETA: 3s - loss: 0.0179 - acc: 0.995 - ETA: 3s - loss: 0.0178 - acc: 0.995 - ETA: 3s - loss: 0.0177 - acc: 0.995 - ETA: 3s - loss: 0.0176 - acc: 0.995 - ETA: 2s - loss: 0.0175 - acc: 0.995 - ETA: 2s - loss: 0.0175 - acc: 0.995 - ETA: 2s - loss: 0.0174 - acc: 0.995 - ETA: 2s - loss: 0.0172 - acc: 0.995 - ETA: 2s - loss: 0.0172 - acc: 0.995 - ETA: 2s - loss: 0.0172 - acc: 0.995 - ETA: 2s - loss: 0.0170 - acc: 0.995 - ETA: 2s - loss: 0.0169 - acc: 0.995 - ETA: 2s - loss: 0.0167 - acc: 0.996 - ETA: 2s - loss: 0.0181 - acc: 0.995 - ETA: 2s - loss: 0.0180 - acc: 0.995 - ETA: 2s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0177 - acc: 0.995 - ETA: 2s - loss: 0.0176 - acc: 0.995 - ETA: 2s - loss: 0.0174 - acc: 0.995 - ETA: 2s - loss: 0.0173 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.996 - ETA: 1s - loss: 0.0171 - acc: 0.996 - ETA: 1s - loss: 0.0170 - acc: 0.996 - ETA: 1s - loss: 0.0169 - acc: 0.996 - ETA: 1s - loss: 0.0167 - acc: 0.996 - ETA: 1s - loss: 0.0166 - acc: 0.996 - ETA: 1s - loss: 0.0165 - acc: 0.996 - ETA: 1s - loss: 0.0164 - acc: 0.996 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0162 - acc: 0.996 - ETA: 1s - loss: 0.0167 - acc: 0.996 - ETA: 1s - loss: 0.0166 - acc: 0.996 - ETA: 1s - loss: 0.0165 - acc: 0.996 - ETA: 1s - loss: 0.0164 - acc: 0.996 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0162 - acc: 0.996 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.9952\n",
      "Epoch 00002: val_acc did not improve from 0.98387\n",
      "5014/5014 [==============================] - 10s 2ms/sample - loss: 0.0177 - acc: 0.9950 - val_loss: 0.0500 - val_acc: 0.9839\n",
      "Epoch 3/200\n",
      "4992/5014 [============================>.] - ETA: 9s - loss: 0.0119 - acc: 1.000 - ETA: 8s - loss: 0.0081 - acc: 1.000 - ETA: 9s - loss: 0.0269 - acc: 0.989 - ETA: 8s - loss: 0.0211 - acc: 0.992 - ETA: 8s - loss: 0.0277 - acc: 0.987 - ETA: 8s - loss: 0.0278 - acc: 0.989 - ETA: 8s - loss: 0.0291 - acc: 0.986 - ETA: 8s - loss: 0.0261 - acc: 0.988 - ETA: 8s - loss: 0.0236 - acc: 0.989 - ETA: 8s - loss: 0.0247 - acc: 0.990 - ETA: 8s - loss: 0.0236 - acc: 0.991 - ETA: 8s - loss: 0.0216 - acc: 0.992 - ETA: 8s - loss: 0.0200 - acc: 0.992 - ETA: 8s - loss: 0.0189 - acc: 0.993 - ETA: 8s - loss: 0.0186 - acc: 0.993 - ETA: 8s - loss: 0.0175 - acc: 0.994 - ETA: 8s - loss: 0.0165 - acc: 0.994 - ETA: 8s - loss: 0.0156 - acc: 0.994 - ETA: 7s - loss: 0.0149 - acc: 0.995 - ETA: 7s - loss: 0.0142 - acc: 0.995 - ETA: 7s - loss: 0.0135 - acc: 0.995 - ETA: 7s - loss: 0.0129 - acc: 0.995 - ETA: 7s - loss: 0.0124 - acc: 0.995 - ETA: 7s - loss: 0.0146 - acc: 0.994 - ETA: 7s - loss: 0.0140 - acc: 0.995 - ETA: 7s - loss: 0.0135 - acc: 0.995 - ETA: 7s - loss: 0.0131 - acc: 0.995 - ETA: 7s - loss: 0.0126 - acc: 0.995 - ETA: 7s - loss: 0.0180 - acc: 0.994 - ETA: 7s - loss: 0.0174 - acc: 0.994 - ETA: 7s - loss: 0.0170 - acc: 0.995 - ETA: 7s - loss: 0.0165 - acc: 0.995 - ETA: 7s - loss: 0.0160 - acc: 0.995 - ETA: 7s - loss: 0.0155 - acc: 0.995 - ETA: 7s - loss: 0.0151 - acc: 0.995 - ETA: 6s - loss: 0.0151 - acc: 0.995 - ETA: 6s - loss: 0.0147 - acc: 0.995 - ETA: 6s - loss: 0.0145 - acc: 0.995 - ETA: 6s - loss: 0.0143 - acc: 0.996 - ETA: 6s - loss: 0.0142 - acc: 0.996 - ETA: 6s - loss: 0.0139 - acc: 0.996 - ETA: 6s - loss: 0.0165 - acc: 0.995 - ETA: 6s - loss: 0.0162 - acc: 0.995 - ETA: 6s - loss: 0.0159 - acc: 0.995 - ETA: 6s - loss: 0.0156 - acc: 0.995 - ETA: 6s - loss: 0.0153 - acc: 0.995 - ETA: 6s - loss: 0.0150 - acc: 0.996 - ETA: 6s - loss: 0.0147 - acc: 0.996 - ETA: 6s - loss: 0.0145 - acc: 0.996 - ETA: 6s - loss: 0.0143 - acc: 0.996 - ETA: 6s - loss: 0.0141 - acc: 0.996 - ETA: 6s - loss: 0.0138 - acc: 0.996 - ETA: 6s - loss: 0.0136 - acc: 0.996 - ETA: 5s - loss: 0.0134 - acc: 0.996 - ETA: 5s - loss: 0.0132 - acc: 0.996 - ETA: 5s - loss: 0.0131 - acc: 0.996 - ETA: 5s - loss: 0.0130 - acc: 0.996 - ETA: 5s - loss: 0.0132 - acc: 0.996 - ETA: 5s - loss: 0.0130 - acc: 0.996 - ETA: 5s - loss: 0.0128 - acc: 0.996 - ETA: 5s - loss: 0.0126 - acc: 0.996 - ETA: 5s - loss: 0.0124 - acc: 0.996 - ETA: 5s - loss: 0.0122 - acc: 0.996 - ETA: 5s - loss: 0.0121 - acc: 0.996 - ETA: 5s - loss: 0.0121 - acc: 0.996 - ETA: 5s - loss: 0.0119 - acc: 0.996 - ETA: 5s - loss: 0.0118 - acc: 0.996 - ETA: 5s - loss: 0.0116 - acc: 0.996 - ETA: 5s - loss: 0.0114 - acc: 0.996 - ETA: 5s - loss: 0.0113 - acc: 0.996 - ETA: 5s - loss: 0.0111 - acc: 0.996 - ETA: 5s - loss: 0.0111 - acc: 0.997 - ETA: 4s - loss: 0.0110 - acc: 0.997 - ETA: 4s - loss: 0.0121 - acc: 0.996 - ETA: 4s - loss: 0.0120 - acc: 0.996 - ETA: 4s - loss: 0.0128 - acc: 0.996 - ETA: 4s - loss: 0.0126 - acc: 0.996 - ETA: 4s - loss: 0.0125 - acc: 0.996 - ETA: 4s - loss: 0.0151 - acc: 0.996 - ETA: 4s - loss: 0.0149 - acc: 0.996 - ETA: 4s - loss: 0.0147 - acc: 0.996 - ETA: 4s - loss: 0.0146 - acc: 0.996 - ETA: 4s - loss: 0.0145 - acc: 0.996 - ETA: 4s - loss: 0.0148 - acc: 0.995 - ETA: 4s - loss: 0.0147 - acc: 0.996 - ETA: 4s - loss: 0.0154 - acc: 0.995 - ETA: 4s - loss: 0.0153 - acc: 0.995 - ETA: 4s - loss: 0.0153 - acc: 0.995 - ETA: 4s - loss: 0.0152 - acc: 0.995 - ETA: 3s - loss: 0.0150 - acc: 0.995 - ETA: 3s - loss: 0.0149 - acc: 0.995 - ETA: 3s - loss: 0.0147 - acc: 0.995 - ETA: 3s - loss: 0.0146 - acc: 0.996 - ETA: 3s - loss: 0.0146 - acc: 0.996 - ETA: 3s - loss: 0.0144 - acc: 0.996 - ETA: 3s - loss: 0.0143 - acc: 0.996 - ETA: 3s - loss: 0.0142 - acc: 0.996 - ETA: 3s - loss: 0.0141 - acc: 0.996 - ETA: 3s - loss: 0.0140 - acc: 0.996 - ETA: 3s - loss: 0.0138 - acc: 0.996 - ETA: 3s - loss: 0.0137 - acc: 0.996 - ETA: 3s - loss: 0.0136 - acc: 0.996 - ETA: 3s - loss: 0.0135 - acc: 0.996 - ETA: 3s - loss: 0.0134 - acc: 0.996 - ETA: 3s - loss: 0.0133 - acc: 0.996 - ETA: 3s - loss: 0.0132 - acc: 0.996 - ETA: 2s - loss: 0.0131 - acc: 0.996 - ETA: 2s - loss: 0.0129 - acc: 0.996 - ETA: 2s - loss: 0.0128 - acc: 0.996 - ETA: 2s - loss: 0.0127 - acc: 0.996 - ETA: 2s - loss: 0.0126 - acc: 0.996 - ETA: 2s - loss: 0.0125 - acc: 0.996 - ETA: 2s - loss: 0.0124 - acc: 0.996 - ETA: 2s - loss: 0.0123 - acc: 0.996 - ETA: 2s - loss: 0.0122 - acc: 0.996 - ETA: 2s - loss: 0.0121 - acc: 0.996 - ETA: 2s - loss: 0.0120 - acc: 0.996 - ETA: 2s - loss: 0.0119 - acc: 0.996 - ETA: 2s - loss: 0.0118 - acc: 0.996 - ETA: 2s - loss: 0.0117 - acc: 0.996 - ETA: 2s - loss: 0.0116 - acc: 0.996 - ETA: 2s - loss: 0.0120 - acc: 0.996 - ETA: 2s - loss: 0.0119 - acc: 0.996 - ETA: 1s - loss: 0.0118 - acc: 0.996 - ETA: 1s - loss: 0.0118 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0113 - acc: 0.996 - ETA: 1s - loss: 0.0112 - acc: 0.996 - ETA: 1s - loss: 0.0111 - acc: 0.996 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.997 - ETA: 0s - loss: 0.0111 - acc: 0.997 - ETA: 0s - loss: 0.0111 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0109 - acc: 0.997 - ETA: 0s - loss: 0.0109 - acc: 0.997 - ETA: 0s - loss: 0.0109 - acc: 0.997 - ETA: 0s - loss: 0.0109 - acc: 0.997 - ETA: 0s - loss: 0.0108 - acc: 0.997 - ETA: 0s - loss: 0.0107 - acc: 0.997 - ETA: 0s - loss: 0.0107 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.9972\n",
      "Epoch 00003: val_acc improved from 0.98387 to 0.98566, saving model to SPAM_CLASSIFIER\n",
      "5014/5014 [==============================] - 10s 2ms/sample - loss: 0.0106 - acc: 0.9972 - val_loss: 0.0664 - val_acc: 0.9857\n",
      "Epoch 4/200\n",
      "4992/5014 [============================>.] - ETA: 8s - loss: 0.0039 - acc: 1.000 - ETA: 8s - loss: 0.0021 - acc: 1.000 - ETA: 8s - loss: 0.0018 - acc: 1.000 - ETA: 8s - loss: 0.0014 - acc: 1.000 - ETA: 8s - loss: 0.0012 - acc: 1.000 - ETA: 8s - loss: 0.0010 - acc: 1.000 - ETA: 8s - loss: 9.0288e-04 - acc: 1.000 - ETA: 8s - loss: 8.0456e-04 - acc: 1.000 - ETA: 8s - loss: 8.0750e-04 - acc: 1.000 - ETA: 8s - loss: 7.7518e-04 - acc: 1.000 - ETA: 8s - loss: 7.3244e-04 - acc: 1.000 - ETA: 8s - loss: 0.0013 - acc: 1.0000    - ETA: 8s - loss: 0.0012 - acc: 1.000 - ETA: 8s - loss: 0.0012 - acc: 1.000 - ETA: 8s - loss: 0.0011 - acc: 1.000 - ETA: 8s - loss: 0.0011 - acc: 1.000 - ETA: 8s - loss: 0.0010 - acc: 1.000 - ETA: 8s - loss: 0.0010 - acc: 1.000 - ETA: 8s - loss: 0.0013 - acc: 1.000 - ETA: 8s - loss: 0.0013 - acc: 1.000 - ETA: 8s - loss: 0.0012 - acc: 1.000 - ETA: 8s - loss: 0.0012 - acc: 1.000 - ETA: 8s - loss: 0.0011 - acc: 1.000 - ETA: 8s - loss: 0.0011 - acc: 1.000 - ETA: 7s - loss: 0.0011 - acc: 1.000 - ETA: 7s - loss: 0.0011 - acc: 1.000 - ETA: 7s - loss: 0.0010 - acc: 1.000 - ETA: 7s - loss: 0.0010 - acc: 1.000 - ETA: 7s - loss: 9.8629e-04 - acc: 1.000 - ETA: 7s - loss: 9.5648e-04 - acc: 1.000 - ETA: 7s - loss: 9.2900e-04 - acc: 1.000 - ETA: 7s - loss: 9.1341e-04 - acc: 1.000 - ETA: 7s - loss: 9.3696e-04 - acc: 1.000 - ETA: 7s - loss: 9.1637e-04 - acc: 1.000 - ETA: 7s - loss: 8.9711e-04 - acc: 1.000 - ETA: 7s - loss: 0.0026 - acc: 0.9991    - ETA: 7s - loss: 0.0026 - acc: 0.999 - ETA: 7s - loss: 0.0025 - acc: 0.999 - ETA: 7s - loss: 0.0025 - acc: 0.999 - ETA: 7s - loss: 0.0024 - acc: 0.999 - ETA: 7s - loss: 0.0024 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0025 - acc: 0.999 - ETA: 6s - loss: 0.0025 - acc: 0.999 - ETA: 6s - loss: 0.0025 - acc: 0.999 - ETA: 6s - loss: 0.0024 - acc: 0.999 - ETA: 6s - loss: 0.0024 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0022 - acc: 0.999 - ETA: 6s - loss: 0.0022 - acc: 0.999 - ETA: 6s - loss: 0.0022 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0022 - acc: 0.999 - ETA: 5s - loss: 0.0022 - acc: 0.999 - ETA: 5s - loss: 0.0022 - acc: 0.999 - ETA: 5s - loss: 0.0022 - acc: 0.999 - ETA: 5s - loss: 0.0021 - acc: 0.999 - ETA: 5s - loss: 0.0021 - acc: 0.999 - ETA: 5s - loss: 0.0021 - acc: 0.999 - ETA: 5s - loss: 0.0021 - acc: 0.999 - ETA: 5s - loss: 0.0020 - acc: 0.999 - ETA: 5s - loss: 0.0020 - acc: 0.999 - ETA: 5s - loss: 0.0020 - acc: 0.999 - ETA: 5s - loss: 0.0020 - acc: 0.999 - ETA: 5s - loss: 0.0019 - acc: 0.999 - ETA: 5s - loss: 0.0019 - acc: 0.999 - ETA: 5s - loss: 0.0019 - acc: 0.999 - ETA: 5s - loss: 0.0019 - acc: 0.999 - ETA: 5s - loss: 0.0018 - acc: 0.999 - ETA: 5s - loss: 0.0019 - acc: 0.999 - ETA: 4s - loss: 0.0019 - acc: 0.999 - ETA: 4s - loss: 0.0018 - acc: 0.999 - ETA: 4s - loss: 0.0018 - acc: 0.999 - ETA: 4s - loss: 0.0018 - acc: 0.999 - ETA: 4s - loss: 0.0018 - acc: 0.999 - ETA: 4s - loss: 0.0018 - acc: 0.999 - ETA: 4s - loss: 0.0017 - acc: 0.999 - ETA: 4s - loss: 0.0017 - acc: 0.999 - ETA: 4s - loss: 0.0017 - acc: 0.999 - ETA: 4s - loss: 0.0018 - acc: 0.999 - ETA: 4s - loss: 0.0018 - acc: 0.999 - ETA: 4s - loss: 0.0017 - acc: 0.999 - ETA: 4s - loss: 0.0017 - acc: 0.999 - ETA: 4s - loss: 0.0017 - acc: 0.999 - ETA: 4s - loss: 0.0017 - acc: 0.999 - ETA: 4s - loss: 0.0018 - acc: 0.999 - ETA: 3s - loss: 0.0018 - acc: 0.999 - ETA: 3s - loss: 0.0018 - acc: 0.999 - ETA: 3s - loss: 0.0018 - acc: 0.999 - ETA: 3s - loss: 0.0018 - acc: 0.999 - ETA: 3s - loss: 0.0018 - acc: 0.999 - ETA: 3s - loss: 0.0017 - acc: 0.999 - ETA: 3s - loss: 0.0017 - acc: 0.999 - ETA: 3s - loss: 0.0017 - acc: 0.999 - ETA: 3s - loss: 0.0017 - acc: 0.999 - ETA: 3s - loss: 0.0017 - acc: 0.999 - ETA: 3s - loss: 0.0017 - acc: 0.999 - ETA: 3s - loss: 0.0019 - acc: 0.999 - ETA: 3s - loss: 0.0019 - acc: 0.999 - ETA: 3s - loss: 0.0019 - acc: 0.999 - ETA: 3s - loss: 0.0019 - acc: 0.999 - ETA: 3s - loss: 0.0019 - acc: 0.999 - ETA: 2s - loss: 0.0020 - acc: 0.999 - ETA: 2s - loss: 0.0020 - acc: 0.999 - ETA: 2s - loss: 0.0020 - acc: 0.999 - ETA: 2s - loss: 0.0022 - acc: 0.999 - ETA: 2s - loss: 0.0022 - acc: 0.999 - ETA: 2s - loss: 0.0024 - acc: 0.999 - ETA: 2s - loss: 0.0024 - acc: 0.999 - ETA: 2s - loss: 0.0024 - acc: 0.999 - ETA: 2s - loss: 0.0024 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 1s - loss: 0.0023 - acc: 0.999 - ETA: 1s - loss: 0.0023 - acc: 0.999 - ETA: 1s - loss: 0.0023 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0023 - acc: 0.999 - ETA: 1s - loss: 0.0023 - acc: 0.999 - ETA: 1s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.9992\n",
      "Epoch 00004: val_acc did not improve from 0.98566\n",
      "5014/5014 [==============================] - 10s 2ms/sample - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0931 - val_acc: 0.9839\n",
      "Epoch 5/200\n",
      "4992/5014 [============================>.] - ETA: 9s - loss: 4.9706e-05 - acc: 1.000 - ETA: 10s - loss: 6.0360e-04 - acc: 1.00 - ETA: 10s - loss: 4.4805e-04 - acc: 1.00 - ETA: 10s - loss: 3.5659e-04 - acc: 1.00 - ETA: 9s - loss: 2.9801e-04 - acc: 1.0000 - ETA: 9s - loss: 2.6334e-04 - acc: 1.000 - ETA: 9s - loss: 3.9594e-04 - acc: 1.000 - ETA: 9s - loss: 3.5084e-04 - acc: 1.000 - ETA: 9s - loss: 3.1709e-04 - acc: 1.000 - ETA: 9s - loss: 2.9280e-04 - acc: 1.000 - ETA: 9s - loss: 2.7684e-04 - acc: 1.000 - ETA: 8s - loss: 2.9618e-04 - acc: 1.000 - ETA: 8s - loss: 2.8108e-04 - acc: 1.000 - ETA: 8s - loss: 2.6440e-04 - acc: 1.000 - ETA: 8s - loss: 2.5442e-04 - acc: 1.000 - ETA: 8s - loss: 2.4290e-04 - acc: 1.000 - ETA: 8s - loss: 2.3260e-04 - acc: 1.000 - ETA: 8s - loss: 2.2102e-04 - acc: 1.000 - ETA: 9s - loss: 2.1400e-04 - acc: 1.000 - ETA: 9s - loss: 2.0659e-04 - acc: 1.000 - ETA: 9s - loss: 1.9854e-04 - acc: 1.000 - ETA: 9s - loss: 1.9084e-04 - acc: 1.000 - ETA: 9s - loss: 1.8474e-04 - acc: 1.000 - ETA: 9s - loss: 1.7864e-04 - acc: 1.000 - ETA: 9s - loss: 1.7243e-04 - acc: 1.000 - ETA: 9s - loss: 1.6702e-04 - acc: 1.000 - ETA: 9s - loss: 1.6500e-04 - acc: 1.000 - ETA: 9s - loss: 1.6018e-04 - acc: 1.000 - ETA: 9s - loss: 1.8898e-04 - acc: 1.000 - ETA: 9s - loss: 1.8461e-04 - acc: 1.000 - ETA: 8s - loss: 1.8054e-04 - acc: 1.000 - ETA: 8s - loss: 1.7620e-04 - acc: 1.000 - ETA: 8s - loss: 1.7116e-04 - acc: 1.000 - ETA: 8s - loss: 1.8419e-04 - acc: 1.000 - ETA: 8s - loss: 1.8280e-04 - acc: 1.000 - ETA: 8s - loss: 1.9451e-04 - acc: 1.000 - ETA: 8s - loss: 1.9123e-04 - acc: 1.000 - ETA: 8s - loss: 1.8773e-04 - acc: 1.000 - ETA: 8s - loss: 1.8423e-04 - acc: 1.000 - ETA: 8s - loss: 1.8233e-04 - acc: 1.000 - ETA: 8s - loss: 1.7945e-04 - acc: 1.000 - ETA: 8s - loss: 1.7757e-04 - acc: 1.000 - ETA: 8s - loss: 1.7442e-04 - acc: 1.000 - ETA: 8s - loss: 1.7962e-04 - acc: 1.000 - ETA: 8s - loss: 1.7749e-04 - acc: 1.000 - ETA: 8s - loss: 1.7439e-04 - acc: 1.000 - ETA: 8s - loss: 1.7148e-04 - acc: 1.000 - ETA: 8s - loss: 1.6827e-04 - acc: 1.000 - ETA: 8s - loss: 1.6553e-04 - acc: 1.000 - ETA: 8s - loss: 1.6257e-04 - acc: 1.000 - ETA: 8s - loss: 1.6085e-04 - acc: 1.000 - ETA: 7s - loss: 1.6615e-04 - acc: 1.000 - ETA: 7s - loss: 1.6359e-04 - acc: 1.000 - ETA: 7s - loss: 1.6089e-04 - acc: 1.000 - ETA: 7s - loss: 1.5890e-04 - acc: 1.000 - ETA: 7s - loss: 1.5671e-04 - acc: 1.000 - ETA: 7s - loss: 1.5439e-04 - acc: 1.000 - ETA: 7s - loss: 1.5337e-04 - acc: 1.000 - ETA: 7s - loss: 1.5142e-04 - acc: 1.000 - ETA: 7s - loss: 1.4990e-04 - acc: 1.000 - ETA: 7s - loss: 1.4813e-04 - acc: 1.000 - ETA: 7s - loss: 1.4908e-04 - acc: 1.000 - ETA: 7s - loss: 1.4715e-04 - acc: 1.000 - ETA: 7s - loss: 1.4543e-04 - acc: 1.000 - ETA: 7s - loss: 1.4345e-04 - acc: 1.000 - ETA: 7s - loss: 1.4605e-04 - acc: 1.000 - ETA: 7s - loss: 1.4428e-04 - acc: 1.000 - ETA: 6s - loss: 1.7882e-04 - acc: 1.000 - ETA: 6s - loss: 1.8192e-04 - acc: 1.000 - ETA: 6s - loss: 1.8014e-04 - acc: 1.000 - ETA: 6s - loss: 1.7779e-04 - acc: 1.000 - ETA: 6s - loss: 1.7619e-04 - acc: 1.000 - ETA: 6s - loss: 1.7439e-04 - acc: 1.000 - ETA: 6s - loss: 1.7266e-04 - acc: 1.000 - ETA: 6s - loss: 1.7074e-04 - acc: 1.000 - ETA: 6s - loss: 1.6879e-04 - acc: 1.000 - ETA: 6s - loss: 1.6684e-04 - acc: 1.000 - ETA: 6s - loss: 1.6580e-04 - acc: 1.000 - ETA: 6s - loss: 1.6403e-04 - acc: 1.000 - ETA: 6s - loss: 1.6813e-04 - acc: 1.000 - ETA: 5s - loss: 1.6972e-04 - acc: 1.000 - ETA: 5s - loss: 1.6829e-04 - acc: 1.000 - ETA: 5s - loss: 1.6701e-04 - acc: 1.000 - ETA: 5s - loss: 1.6533e-04 - acc: 1.000 - ETA: 5s - loss: 1.6366e-04 - acc: 1.000 - ETA: 5s - loss: 1.6191e-04 - acc: 1.000 - ETA: 5s - loss: 1.6014e-04 - acc: 1.000 - ETA: 5s - loss: 1.5870e-04 - acc: 1.000 - ETA: 5s - loss: 1.5717e-04 - acc: 1.000 - ETA: 5s - loss: 1.5623e-04 - acc: 1.000 - ETA: 5s - loss: 1.5487e-04 - acc: 1.000 - ETA: 5s - loss: 1.5342e-04 - acc: 1.000 - ETA: 5s - loss: 1.5183e-04 - acc: 1.000 - ETA: 5s - loss: 1.5043e-04 - acc: 1.000 - ETA: 4s - loss: 1.5028e-04 - acc: 1.000 - ETA: 4s - loss: 1.4954e-04 - acc: 1.000 - ETA: 4s - loss: 1.4829e-04 - acc: 1.000 - ETA: 4s - loss: 1.4710e-04 - acc: 1.000 - ETA: 4s - loss: 1.4645e-04 - acc: 1.000 - ETA: 4s - loss: 1.4515e-04 - acc: 1.000 - ETA: 4s - loss: 1.4451e-04 - acc: 1.000 - ETA: 4s - loss: 1.4354e-04 - acc: 1.000 - ETA: 4s - loss: 1.4225e-04 - acc: 1.000 - ETA: 4s - loss: 1.4131e-04 - acc: 1.000 - ETA: 4s - loss: 1.4437e-04 - acc: 1.000 - ETA: 4s - loss: 1.4431e-04 - acc: 1.000 - ETA: 3s - loss: 1.4316e-04 - acc: 1.000 - ETA: 3s - loss: 1.4196e-04 - acc: 1.000 - ETA: 3s - loss: 1.4072e-04 - acc: 1.000 - ETA: 3s - loss: 1.3959e-04 - acc: 1.000 - ETA: 3s - loss: 1.3861e-04 - acc: 1.000 - ETA: 3s - loss: 1.3766e-04 - acc: 1.000 - ETA: 3s - loss: 1.3649e-04 - acc: 1.000 - ETA: 3s - loss: 1.3553e-04 - acc: 1.000 - ETA: 3s - loss: 1.3451e-04 - acc: 1.000 - ETA: 3s - loss: 1.3341e-04 - acc: 1.000 - ETA: 3s - loss: 1.4361e-04 - acc: 1.000 - ETA: 3s - loss: 1.4248e-04 - acc: 1.000 - ETA: 3s - loss: 1.4185e-04 - acc: 1.000 - ETA: 2s - loss: 1.4074e-04 - acc: 1.000 - ETA: 2s - loss: 1.3990e-04 - acc: 1.000 - ETA: 2s - loss: 1.3923e-04 - acc: 1.000 - ETA: 2s - loss: 1.3819e-04 - acc: 1.000 - ETA: 2s - loss: 1.3821e-04 - acc: 1.000 - ETA: 2s - loss: 1.3720e-04 - acc: 1.000 - ETA: 2s - loss: 1.3626e-04 - acc: 1.000 - ETA: 2s - loss: 1.3538e-04 - acc: 1.000 - ETA: 2s - loss: 1.3451e-04 - acc: 1.000 - ETA: 2s - loss: 1.3369e-04 - acc: 1.000 - ETA: 2s - loss: 1.3279e-04 - acc: 1.000 - ETA: 2s - loss: 1.3187e-04 - acc: 1.000 - ETA: 1s - loss: 1.3095e-04 - acc: 1.000 - ETA: 1s - loss: 5.6011e-04 - acc: 0.999 - ETA: 1s - loss: 5.5626e-04 - acc: 0.999 - ETA: 1s - loss: 5.5265e-04 - acc: 0.999 - ETA: 1s - loss: 5.4999e-04 - acc: 0.999 - ETA: 1s - loss: 5.9791e-04 - acc: 0.999 - ETA: 1s - loss: 5.9398e-04 - acc: 0.999 - ETA: 1s - loss: 5.9106e-04 - acc: 0.999 - ETA: 1s - loss: 5.8765e-04 - acc: 0.999 - ETA: 1s - loss: 5.8487e-04 - acc: 0.999 - ETA: 1s - loss: 8.4999e-04 - acc: 0.999 - ETA: 1s - loss: 8.4850e-04 - acc: 0.999 - ETA: 1s - loss: 8.4587e-04 - acc: 0.999 - ETA: 0s - loss: 8.7164e-04 - acc: 0.999 - ETA: 0s - loss: 8.6700e-04 - acc: 0.999 - ETA: 0s - loss: 8.6441e-04 - acc: 0.999 - ETA: 0s - loss: 8.7570e-04 - acc: 0.999 - ETA: 0s - loss: 9.1734e-04 - acc: 0.999 - ETA: 0s - loss: 9.2111e-04 - acc: 0.999 - ETA: 0s - loss: 9.2007e-04 - acc: 0.999 - ETA: 0s - loss: 9.2011e-04 - acc: 0.999 - ETA: 0s - loss: 9.1594e-04 - acc: 0.999 - ETA: 0s - loss: 9.3389e-04 - acc: 0.999 - ETA: 0s - loss: 9.3064e-04 - acc: 0.999 - ETA: 0s - loss: 9.3277e-04 - acc: 0.9996\n",
      "Epoch 00005: val_acc did not improve from 0.98566\n",
      "5014/5014 [==============================] - 13s 3ms/sample - loss: 9.2957e-04 - acc: 0.9996 - val_loss: 0.0939 - val_acc: 0.9803\n",
      "Epoch 6/200\n",
      "4992/5014 [============================>.] - ETA: 11s - loss: 4.9390e-04 - acc: 1.00 - ETA: 11s - loss: 3.9573e-04 - acc: 1.00 - ETA: 11s - loss: 3.8619e-04 - acc: 1.00 - ETA: 12s - loss: 7.3441e-04 - acc: 1.00 - ETA: 11s - loss: 8.3342e-04 - acc: 1.00 - ETA: 11s - loss: 7.5768e-04 - acc: 1.00 - ETA: 11s - loss: 7.6310e-04 - acc: 1.00 - ETA: 11s - loss: 6.9115e-04 - acc: 1.00 - ETA: 11s - loss: 6.4122e-04 - acc: 1.00 - ETA: 11s - loss: 6.3740e-04 - acc: 1.00 - ETA: 11s - loss: 6.0394e-04 - acc: 1.00 - ETA: 11s - loss: 5.5996e-04 - acc: 1.00 - ETA: 11s - loss: 5.3817e-04 - acc: 1.00 - ETA: 11s - loss: 5.1453e-04 - acc: 1.00 - ETA: 10s - loss: 5.1167e-04 - acc: 1.00 - ETA: 10s - loss: 4.9771e-04 - acc: 1.00 - ETA: 10s - loss: 4.8443e-04 - acc: 1.00 - ETA: 10s - loss: 4.8399e-04 - acc: 1.00 - ETA: 10s - loss: 4.6450e-04 - acc: 1.00 - ETA: 10s - loss: 4.6262e-04 - acc: 1.00 - ETA: 10s - loss: 4.4883e-04 - acc: 1.00 - ETA: 10s - loss: 4.3846e-04 - acc: 1.00 - ETA: 10s - loss: 4.9035e-04 - acc: 1.00 - ETA: 10s - loss: 4.7391e-04 - acc: 1.00 - ETA: 10s - loss: 4.7826e-04 - acc: 1.00 - ETA: 10s - loss: 5.1477e-04 - acc: 1.00 - ETA: 10s - loss: 5.0434e-04 - acc: 1.00 - ETA: 10s - loss: 5.1160e-04 - acc: 1.00 - ETA: 10s - loss: 4.9654e-04 - acc: 1.00 - ETA: 10s - loss: 4.8855e-04 - acc: 1.00 - ETA: 9s - loss: 4.7590e-04 - acc: 1.0000 - ETA: 9s - loss: 4.6708e-04 - acc: 1.000 - ETA: 9s - loss: 4.7354e-04 - acc: 1.000 - ETA: 9s - loss: 4.6819e-04 - acc: 1.000 - ETA: 9s - loss: 4.5749e-04 - acc: 1.000 - ETA: 9s - loss: 4.4950e-04 - acc: 1.000 - ETA: 9s - loss: 4.4001e-04 - acc: 1.000 - ETA: 9s - loss: 4.3273e-04 - acc: 1.000 - ETA: 9s - loss: 4.2425e-04 - acc: 1.000 - ETA: 9s - loss: 4.2582e-04 - acc: 1.000 - ETA: 9s - loss: 4.1659e-04 - acc: 1.000 - ETA: 9s - loss: 4.0851e-04 - acc: 1.000 - ETA: 9s - loss: 4.0735e-04 - acc: 1.000 - ETA: 9s - loss: 4.0925e-04 - acc: 1.000 - ETA: 8s - loss: 4.0199e-04 - acc: 1.000 - ETA: 8s - loss: 3.9559e-04 - acc: 1.000 - ETA: 8s - loss: 3.9088e-04 - acc: 1.000 - ETA: 8s - loss: 3.8451e-04 - acc: 1.000 - ETA: 8s - loss: 3.7834e-04 - acc: 1.000 - ETA: 8s - loss: 3.7139e-04 - acc: 1.000 - ETA: 8s - loss: 3.6557e-04 - acc: 1.000 - ETA: 8s - loss: 3.6059e-04 - acc: 1.000 - ETA: 8s - loss: 3.5593e-04 - acc: 1.000 - ETA: 8s - loss: 3.5080e-04 - acc: 1.000 - ETA: 8s - loss: 3.4774e-04 - acc: 1.000 - ETA: 8s - loss: 3.4412e-04 - acc: 1.000 - ETA: 7s - loss: 3.3871e-04 - acc: 1.000 - ETA: 7s - loss: 3.3429e-04 - acc: 1.000 - ETA: 7s - loss: 3.4224e-04 - acc: 1.000 - ETA: 7s - loss: 3.3906e-04 - acc: 1.000 - ETA: 7s - loss: 3.3415e-04 - acc: 1.000 - ETA: 7s - loss: 3.3353e-04 - acc: 1.000 - ETA: 7s - loss: 3.3073e-04 - acc: 1.000 - ETA: 7s - loss: 3.2720e-04 - acc: 1.000 - ETA: 7s - loss: 3.3166e-04 - acc: 1.000 - ETA: 7s - loss: 3.2748e-04 - acc: 1.000 - ETA: 7s - loss: 3.2364e-04 - acc: 1.000 - ETA: 7s - loss: 0.0021 - acc: 0.9995    - ETA: 7s - loss: 0.0020 - acc: 0.999 - ETA: 6s - loss: 0.0020 - acc: 0.999 - ETA: 6s - loss: 0.0020 - acc: 0.999 - ETA: 6s - loss: 0.0020 - acc: 0.999 - ETA: 6s - loss: 0.0019 - acc: 0.999 - ETA: 6s - loss: 0.0019 - acc: 0.999 - ETA: 6s - loss: 0.0019 - acc: 0.999 - ETA: 6s - loss: 0.0038 - acc: 0.999 - ETA: 6s - loss: 0.0039 - acc: 0.999 - ETA: 6s - loss: 0.0038 - acc: 0.999 - ETA: 6s - loss: 0.0038 - acc: 0.999 - ETA: 6s - loss: 0.0038 - acc: 0.999 - ETA: 6s - loss: 0.0037 - acc: 0.999 - ETA: 6s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0038 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0036 - acc: 0.999 - ETA: 5s - loss: 0.0036 - acc: 0.999 - ETA: 4s - loss: 0.0036 - acc: 0.999 - ETA: 4s - loss: 0.0035 - acc: 0.999 - ETA: 4s - loss: 0.0035 - acc: 0.999 - ETA: 4s - loss: 0.0034 - acc: 0.999 - ETA: 4s - loss: 0.0034 - acc: 0.999 - ETA: 4s - loss: 0.0034 - acc: 0.999 - ETA: 4s - loss: 0.0034 - acc: 0.999 - ETA: 4s - loss: 0.0033 - acc: 0.999 - ETA: 4s - loss: 0.0033 - acc: 0.999 - ETA: 4s - loss: 0.0033 - acc: 0.999 - ETA: 4s - loss: 0.0032 - acc: 0.999 - ETA: 4s - loss: 0.0032 - acc: 0.999 - ETA: 4s - loss: 0.0032 - acc: 0.999 - ETA: 3s - loss: 0.0031 - acc: 0.999 - ETA: 3s - loss: 0.0031 - acc: 0.999 - ETA: 3s - loss: 0.0031 - acc: 0.999 - ETA: 3s - loss: 0.0031 - acc: 0.999 - ETA: 3s - loss: 0.0031 - acc: 0.999 - ETA: 3s - loss: 0.0030 - acc: 0.999 - ETA: 3s - loss: 0.0030 - acc: 0.999 - ETA: 3s - loss: 0.0030 - acc: 0.999 - ETA: 3s - loss: 0.0030 - acc: 0.999 - ETA: 3s - loss: 0.0029 - acc: 0.999 - ETA: 3s - loss: 0.0029 - acc: 0.999 - ETA: 3s - loss: 0.0029 - acc: 0.999 - ETA: 2s - loss: 0.0029 - acc: 0.999 - ETA: 2s - loss: 0.0028 - acc: 0.999 - ETA: 2s - loss: 0.0028 - acc: 0.999 - ETA: 2s - loss: 0.0033 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0033 - acc: 0.999 - ETA: 2s - loss: 0.0033 - acc: 0.999 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.9986\n",
      "Epoch 00006: val_acc did not improve from 0.98566\n",
      "5014/5014 [==============================] - 13s 3ms/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.1098 - val_acc: 0.9821\n",
      "Epoch 7/200\n",
      "4992/5014 [============================>.] - ETA: 11s - loss: 2.0943e-04 - acc: 1.00 - ETA: 10s - loss: 1.9890e-04 - acc: 1.00 - ETA: 10s - loss: 4.9531e-04 - acc: 1.00 - ETA: 10s - loss: 3.9073e-04 - acc: 1.00 - ETA: 10s - loss: 3.8257e-04 - acc: 1.00 - ETA: 10s - loss: 4.8246e-04 - acc: 1.00 - ETA: 10s - loss: 5.4193e-04 - acc: 1.00 - ETA: 10s - loss: 4.9780e-04 - acc: 1.00 - ETA: 10s - loss: 4.6903e-04 - acc: 1.00 - ETA: 10s - loss: 4.2922e-04 - acc: 1.00 - ETA: 10s - loss: 3.9690e-04 - acc: 1.00 - ETA: 10s - loss: 3.7253e-04 - acc: 1.00 - ETA: 10s - loss: 3.5755e-04 - acc: 1.00 - ETA: 10s - loss: 3.4028e-04 - acc: 1.00 - ETA: 10s - loss: 3.2048e-04 - acc: 1.00 - ETA: 10s - loss: 3.1790e-04 - acc: 1.00 - ETA: 10s - loss: 3.1714e-04 - acc: 1.00 - ETA: 10s - loss: 3.0350e-04 - acc: 1.00 - ETA: 10s - loss: 2.9537e-04 - acc: 1.00 - ETA: 10s - loss: 2.9306e-04 - acc: 1.00 - ETA: 10s - loss: 2.8252e-04 - acc: 1.00 - ETA: 10s - loss: 2.7559e-04 - acc: 1.00 - ETA: 10s - loss: 2.6592e-04 - acc: 1.00 - ETA: 9s - loss: 2.6073e-04 - acc: 1.0000 - ETA: 9s - loss: 2.5321e-04 - acc: 1.000 - ETA: 9s - loss: 2.4639e-04 - acc: 1.000 - ETA: 9s - loss: 2.4405e-04 - acc: 1.000 - ETA: 9s - loss: 2.3837e-04 - acc: 1.000 - ETA: 9s - loss: 2.3389e-04 - acc: 1.000 - ETA: 9s - loss: 2.3781e-04 - acc: 1.000 - ETA: 9s - loss: 2.3161e-04 - acc: 1.000 - ETA: 9s - loss: 2.2910e-04 - acc: 1.000 - ETA: 9s - loss: 2.2501e-04 - acc: 1.000 - ETA: 9s - loss: 2.3878e-04 - acc: 1.000 - ETA: 9s - loss: 2.3382e-04 - acc: 1.000 - ETA: 9s - loss: 2.3318e-04 - acc: 1.000 - ETA: 9s - loss: 2.2909e-04 - acc: 1.000 - ETA: 9s - loss: 2.2437e-04 - acc: 1.000 - ETA: 8s - loss: 2.1988e-04 - acc: 1.000 - ETA: 8s - loss: 2.1620e-04 - acc: 1.000 - ETA: 8s - loss: 2.1466e-04 - acc: 1.000 - ETA: 8s - loss: 2.1044e-04 - acc: 1.000 - ETA: 8s - loss: 2.0722e-04 - acc: 1.000 - ETA: 8s - loss: 2.2281e-04 - acc: 1.000 - ETA: 8s - loss: 2.2120e-04 - acc: 1.000 - ETA: 8s - loss: 0.0023 - acc: 0.9993    - ETA: 8s - loss: 0.0022 - acc: 0.999 - ETA: 8s - loss: 0.0023 - acc: 0.999 - ETA: 8s - loss: 0.0022 - acc: 0.999 - ETA: 8s - loss: 0.0022 - acc: 0.999 - ETA: 8s - loss: 0.0028 - acc: 0.999 - ETA: 8s - loss: 0.0029 - acc: 0.999 - ETA: 7s - loss: 0.0028 - acc: 0.999 - ETA: 7s - loss: 0.0028 - acc: 0.999 - ETA: 7s - loss: 0.0028 - acc: 0.999 - ETA: 7s - loss: 0.0027 - acc: 0.999 - ETA: 7s - loss: 0.0027 - acc: 0.999 - ETA: 7s - loss: 0.0027 - acc: 0.999 - ETA: 7s - loss: 0.0026 - acc: 0.999 - ETA: 7s - loss: 0.0026 - acc: 0.999 - ETA: 7s - loss: 0.0026 - acc: 0.999 - ETA: 7s - loss: 0.0025 - acc: 0.999 - ETA: 7s - loss: 0.0025 - acc: 0.999 - ETA: 7s - loss: 0.0025 - acc: 0.999 - ETA: 7s - loss: 0.0025 - acc: 0.999 - ETA: 7s - loss: 0.0027 - acc: 0.999 - ETA: 6s - loss: 0.0027 - acc: 0.999 - ETA: 6s - loss: 0.0026 - acc: 0.999 - ETA: 6s - loss: 0.0026 - acc: 0.999 - ETA: 6s - loss: 0.0026 - acc: 0.999 - ETA: 6s - loss: 0.0026 - acc: 0.999 - ETA: 6s - loss: 0.0030 - acc: 0.999 - ETA: 6s - loss: 0.0030 - acc: 0.999 - ETA: 6s - loss: 0.0044 - acc: 0.998 - ETA: 6s - loss: 0.0043 - acc: 0.998 - ETA: 6s - loss: 0.0043 - acc: 0.998 - ETA: 6s - loss: 0.0042 - acc: 0.998 - ETA: 6s - loss: 0.0043 - acc: 0.998 - ETA: 6s - loss: 0.0043 - acc: 0.998 - ETA: 6s - loss: 0.0042 - acc: 0.998 - ETA: 5s - loss: 0.0057 - acc: 0.998 - ETA: 5s - loss: 0.0056 - acc: 0.998 - ETA: 5s - loss: 0.0056 - acc: 0.998 - ETA: 5s - loss: 0.0055 - acc: 0.998 - ETA: 5s - loss: 0.0055 - acc: 0.998 - ETA: 5s - loss: 0.0054 - acc: 0.998 - ETA: 5s - loss: 0.0057 - acc: 0.998 - ETA: 5s - loss: 0.0063 - acc: 0.997 - ETA: 5s - loss: 0.0062 - acc: 0.997 - ETA: 5s - loss: 0.0062 - acc: 0.997 - ETA: 5s - loss: 0.0061 - acc: 0.997 - ETA: 5s - loss: 0.0061 - acc: 0.998 - ETA: 5s - loss: 0.0060 - acc: 0.998 - ETA: 4s - loss: 0.0066 - acc: 0.997 - ETA: 4s - loss: 0.0065 - acc: 0.997 - ETA: 4s - loss: 0.0065 - acc: 0.997 - ETA: 4s - loss: 0.0064 - acc: 0.997 - ETA: 4s - loss: 0.0064 - acc: 0.997 - ETA: 4s - loss: 0.0063 - acc: 0.997 - ETA: 4s - loss: 0.0062 - acc: 0.997 - ETA: 4s - loss: 0.0062 - acc: 0.997 - ETA: 4s - loss: 0.0061 - acc: 0.997 - ETA: 4s - loss: 0.0061 - acc: 0.997 - ETA: 4s - loss: 0.0060 - acc: 0.997 - ETA: 4s - loss: 0.0060 - acc: 0.997 - ETA: 4s - loss: 0.0060 - acc: 0.997 - ETA: 3s - loss: 0.0059 - acc: 0.998 - ETA: 3s - loss: 0.0060 - acc: 0.998 - ETA: 3s - loss: 0.0060 - acc: 0.998 - ETA: 3s - loss: 0.0059 - acc: 0.998 - ETA: 3s - loss: 0.0059 - acc: 0.998 - ETA: 3s - loss: 0.0058 - acc: 0.998 - ETA: 3s - loss: 0.0058 - acc: 0.998 - ETA: 3s - loss: 0.0057 - acc: 0.998 - ETA: 3s - loss: 0.0057 - acc: 0.998 - ETA: 3s - loss: 0.0056 - acc: 0.998 - ETA: 3s - loss: 0.0056 - acc: 0.998 - ETA: 3s - loss: 0.0055 - acc: 0.998 - ETA: 3s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0054 - acc: 0.998 - ETA: 2s - loss: 0.0054 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0052 - acc: 0.998 - ETA: 2s - loss: 0.0052 - acc: 0.998 - ETA: 2s - loss: 0.0051 - acc: 0.998 - ETA: 2s - loss: 0.0051 - acc: 0.998 - ETA: 2s - loss: 0.0051 - acc: 0.998 - ETA: 2s - loss: 0.0050 - acc: 0.998 - ETA: 2s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.9986\n",
      "Epoch 00007: val_acc did not improve from 0.98566\n",
      "5014/5014 [==============================] - 13s 3ms/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0966 - val_acc: 0.9803\n",
      "Epoch 8/200\n",
      "2304/5014 [============>.................] - ETA: 11s - loss: 5.1818e-05 - acc: 1.00 - ETA: 10s - loss: 6.5515e-05 - acc: 1.00 - ETA: 11s - loss: 7.1150e-05 - acc: 1.00 - ETA: 11s - loss: 6.3142e-05 - acc: 1.00 - ETA: 11s - loss: 5.7285e-05 - acc: 1.00 - ETA: 11s - loss: 5.2142e-05 - acc: 1.00 - ETA: 11s - loss: 6.3338e-05 - acc: 1.00 - ETA: 11s - loss: 7.1608e-05 - acc: 1.00 - ETA: 11s - loss: 9.7487e-05 - acc: 1.00 - ETA: 11s - loss: 1.2725e-04 - acc: 1.00 - ETA: 11s - loss: 1.2877e-04 - acc: 1.00 - ETA: 11s - loss: 1.3189e-04 - acc: 1.00 - ETA: 11s - loss: 1.2646e-04 - acc: 1.00 - ETA: 11s - loss: 1.2494e-04 - acc: 1.00 - ETA: 11s - loss: 1.1731e-04 - acc: 1.00 - ETA: 11s - loss: 1.1173e-04 - acc: 1.00 - ETA: 11s - loss: 1.0561e-04 - acc: 1.00 - ETA: 11s - loss: 1.0125e-04 - acc: 1.00 - ETA: 10s - loss: 1.0642e-04 - acc: 1.00 - ETA: 10s - loss: 1.0203e-04 - acc: 1.00 - ETA: 10s - loss: 9.9127e-05 - acc: 1.00 - ETA: 10s - loss: 9.8818e-05 - acc: 1.00 - ETA: 10s - loss: 9.5484e-05 - acc: 1.00 - ETA: 10s - loss: 9.2248e-05 - acc: 1.00 - ETA: 10s - loss: 9.2313e-05 - acc: 1.00 - ETA: 10s - loss: 9.2282e-05 - acc: 1.00 - ETA: 10s - loss: 9.1285e-05 - acc: 1.00 - ETA: 10s - loss: 1.0589e-04 - acc: 1.00 - ETA: 10s - loss: 1.0302e-04 - acc: 1.00 - ETA: 10s - loss: 1.0021e-04 - acc: 1.00 - ETA: 9s - loss: 9.8165e-05 - acc: 1.0000 - ETA: 9s - loss: 1.0397e-04 - acc: 1.000 - ETA: 9s - loss: 1.0183e-04 - acc: 1.000 - ETA: 9s - loss: 9.9773e-05 - acc: 1.000 - ETA: 9s - loss: 9.8086e-05 - acc: 1.000 - ETA: 9s - loss: 9.6198e-05 - acc: 1.000 - ETA: 9s - loss: 9.4264e-05 - acc: 1.000 - ETA: 9s - loss: 1.0076e-04 - acc: 1.000 - ETA: 9s - loss: 9.8824e-05 - acc: 1.000 - ETA: 9s - loss: 9.8169e-05 - acc: 1.000 - ETA: 9s - loss: 9.6357e-05 - acc: 1.000 - ETA: 8s - loss: 1.0658e-04 - acc: 1.000 - ETA: 8s - loss: 1.0890e-04 - acc: 1.000 - ETA: 8s - loss: 1.0740e-04 - acc: 1.000 - ETA: 8s - loss: 1.0638e-04 - acc: 1.000 - ETA: 8s - loss: 1.0465e-04 - acc: 1.000 - ETA: 8s - loss: 1.0377e-04 - acc: 1.000 - ETA: 8s - loss: 1.0202e-04 - acc: 1.000 - ETA: 8s - loss: 1.0228e-04 - acc: 1.000 - ETA: 8s - loss: 1.1947e-04 - acc: 1.000 - ETA: 8s - loss: 1.1754e-04 - acc: 1.000 - ETA: 8s - loss: 1.1595e-04 - acc: 1.000 - ETA: 8s - loss: 1.1427e-04 - acc: 1.000 - ETA: 8s - loss: 1.1238e-04 - acc: 1.000 - ETA: 7s - loss: 1.1053e-04 - acc: 1.000 - ETA: 7s - loss: 1.0881e-04 - acc: 1.000 - ETA: 7s - loss: 1.0752e-04 - acc: 1.000 - ETA: 7s - loss: 1.0599e-04 - acc: 1.000 - ETA: 7s - loss: 1.0457e-04 - acc: 1.000 - ETA: 7s - loss: 1.0924e-04 - acc: 1.000 - ETA: 7s - loss: 1.0850e-04 - acc: 1.000 - ETA: 7s - loss: 1.0706e-04 - acc: 1.000 - ETA: 7s - loss: 1.0553e-04 - acc: 1.000 - ETA: 7s - loss: 1.0402e-04 - acc: 1.000 - ETA: 7s - loss: 1.0278e-04 - acc: 1.000 - ETA: 7s - loss: 1.0151e-04 - acc: 1.000 - ETA: 7s - loss: 1.0050e-04 - acc: 1.000 - ETA: 6s - loss: 9.9154e-05 - acc: 1.000 - ETA: 6s - loss: 1.1932e-04 - acc: 1.000 - ETA: 6s - loss: 1.1915e-04 - acc: 1.000 - ETA: 6s - loss: 1.1788e-04 - acc: 1.000 - ETA: 6s - loss: 1.1695e-04 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-364076c9260f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                     callbacks = [early_stopping, model_checkpoint])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    validation_data=[x_test, y_test],\n",
    "                    batch_size=32,\n",
    "                    epochs=200,\n",
    "                    callbacks = [early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
