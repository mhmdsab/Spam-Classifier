{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM CLASSIFICATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to follow a step by step procedure to create a spam classifier.\n",
    "This spam classifier uses embeddings trained on the spam classification data set using Fasttext library.\n",
    "These embeddings are then fed to bidirectional LSTM layer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate embedding size and maximum words in words sequence (sentence)\n",
    "embedding_size = 50\n",
    "max_words_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe consists of two columns the v1 and v2 and three other empty columns. v2 column contains the mail body while v1 column contain mail class (ham or spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "df = pd.read_csv('./spam.csv', encoding='latin-1').loc[:,['v1','v2']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we clean the data to be ready for training embeddings\n",
    "def preprocess(df_):\n",
    "    df_cleaned = df_.copy()\n",
    "    #remove nan values\n",
    "    df_cleaned.dropna(inplace = True)\n",
    "    #replace any number by the word number\n",
    "    df_cleaned['v2'] = df_cleaned['v2'].str.replace(r'\\d',' number ')\n",
    "    #remove any punctuations\n",
    "    df_cleaned['v2'] = df_cleaned['v2'].str.replace(r'[^a-zA-Z]', ' ', regex = True)\n",
    "    #remove single characters\n",
    "    df_cleaned['v2'] = df_cleaned['v2'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ', regex = True)\n",
    "    #remove extra spaces\n",
    "    df_cleaned['v2'] = df_cleaned['v2'].str.replace(r'\\s+', ' ', regex = True).map(lambda x:x.lower())\n",
    "    return df_cleaned\n",
    "\n",
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the text cleaned. we create a corpus of the entire dataset in a txt file. This corpus will be fed to a skipgram\n",
    "model to train embeddings. The main advantage of training embeddings using fasttext is that it trains on the entire word \n",
    "and its subwords as well. This minimizes the probability of having out of vocabulary words as in such case, this word will \n",
    "be divided into subwords hopefully present in fasttext model subwords. then the word embedding will be the average of its \n",
    "subwords embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus for training embeddings\n",
    "with open(r'./spamcorpus.txt', 'w', encoding=\"latin-1\") as txtfile:\n",
    "    for i in range(len(df)):\n",
    "        line = df.loc[i,'v2']\n",
    "        txtfile.write(line)\n",
    "        txtfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and train skipgram model using your own custom configurtions\n",
    "model = fasttext.train_unsupervised('./spamcorpus.txt',\n",
    "                                    minCount = 5, \n",
    "                                    model='skipgram',\n",
    "                                    minn = 2,\n",
    "                                    maxn = 5,\n",
    "                                    dim = embedding_size,\n",
    "                                    lr = 0.1,\n",
    "                                    epoch = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a txt file that contains every unique word in the dataset and its embeddings. This can be done by selecting all unique word in the dataset. Then using the trained skigram model, we can have the embeddings for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all unique words in the dataset\n",
    "with open(r'.\\spamcorpus.txt', 'r', encoding=\"utf-8\") as txtfile:\n",
    "    corpus_sentences = txtfile.readlines()\n",
    "    corpus_words = []\n",
    "    for sent in corpus_sentences:\n",
    "        tokenized_sent = sent.split()\n",
    "        for word_ in tokenized_sent:\n",
    "            corpus_words.append(word_)\n",
    "            \n",
    "    corpus_unique_words = list(set(corpus_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embedding txt file\n",
    "with open(r'./fasttext_embeddings.txt', 'w', encoding=\"utf-8\") as txtfile:\n",
    "    txtfile.write(str(len(corpus_unique_words)) + \" \" + str(model.get_dimension()))\n",
    "    txtfile.write('\\n')\n",
    "    for word in corpus_unique_words:\n",
    "        embedding = model.get_word_vector(word)\n",
    "        vstr = \"\"\n",
    "        for vi in embedding:\n",
    "            vstr += \" \" + str(vi)\n",
    "        txtfile.write(word + vstr)\n",
    "        txtfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the previous steps were done to create the words embeddings txt file.\n",
    "As this file is ready now, the data preparation steps will be as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Create an embedding dictionary (keys are unique words, values are embedding arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create a keras tokenizer and fit it on the cleaned text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted tokenizer now has a dictionary of every unique word and its index in a randomly initialized embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: We will create embedding matrix from the created embedding dictionary and will use it instead of the randomly initialized embedding matrix. we will assign every word embedding with its index in the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding matrix is now ready and will be fed directly to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: The tokenizer converts each sequence of words to a sequence of their indices in both tokenizer and embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: sequences with more words than maximum words length (50 in this notebook) are truncated, whereas sequences with less words are padded to maximum words length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: Create an embedding dictionary \n",
    "embedding_dictionary = dict()\n",
    "\n",
    "with open(r'./fasttext_embeddings.txt', 'r', encoding=\"utf-8\") as txtfile:\n",
    "    embeddings = txtfile.readlines()[1:]\n",
    "    for line in embeddings:\n",
    "        x = line.split()\n",
    "        word = x[0]\n",
    "        embeds = np.asarray(x[1:]).astype(np.float32)\n",
    "        embedding_dictionary[word] = embeds\n",
    "    embedding_dictionary['UNK'] = np.mean(list(embedding_dictionary.values()), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Create a keras tokenizer and fit it on the cleaned text.\n",
    "num_words = len(corpus_unique_words)\n",
    "tokenizer = Tokenizer(num_words+1, oov_token = 'UNK')\n",
    "tokenizer.fit_on_texts(df['v2'])\n",
    "\n",
    "#Note1: the number of words in the tokenizer is 1 indexed (index starts from 1)\n",
    "#Note2: we add 1 to the number of words in the tokenizer as it includes the unknown token \n",
    "#Note3: we don't have to add the total number of unique words in the tokenizer, \n",
    "#if we use less number, the tokenizer will account for only the top frequent n words we enter\n",
    "#but I added the total number of words as every single word now hopefully has a meaningful embedding thanks to fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: We will create embedding matrix from the created embedding dictionary\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "embeddings_matrix = np.zeros(shape = (vocab_size , embedding_size))\n",
    "\n",
    "for word, index in tqdm(tokenizer.word_index.items()):\n",
    "    embeddings_matrix[index] = embedding_dictionary.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: The tokenizer converts each sequence of words to a sequence of their indices in both tokenizer and embeddings\n",
    "X = tokenizer.texts_to_sequences(df['v2'])\n",
    "#step 5: padding short sequences and truncating long sequences\n",
    "X = pad_sequences(X, padding = 'post', maxlen = max_words_len, truncating='post')\n",
    "#encoding labels\n",
    "Y = pd.get_dummies(df['v1'])['spam'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a bidirectional LSTM model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_size, embedding_size, \n",
    "                                weights=[embeddings_matrix], \n",
    "                                input_length=max_words_len , \n",
    "                                trainable=True)\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor= 'val_acc', \n",
    "                               mode = 'max',\n",
    "                               patience=30, \n",
    "                               verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('SPAM_CLASSIFIER',\n",
    "                                   monitor = 'val_acc', \n",
    "                                   mode = 'max', \n",
    "                                   save_best_only=True, \n",
    "                                   verbose=1)\n",
    "\n",
    "\n",
    "opt = Adam(lr = 0.01)\n",
    "\n",
    "model.compile(opt, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    validation_data=[x_test, y_test],\n",
    "                    batch_size=32,\n",
    "                    epochs=200,\n",
    "                    callbacks = [early_stopping, model_checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
